{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(observation):\n",
    "    observation = cv2.cvtColor(cv2.resize(observation, (84, 110)), cv2.COLOR_BGR2GRAY)\n",
    "    observation = observation[26:110, :] # removing the first 26 rows as they only contain the score\n",
    "    ret, observation = cv2.threshold(observation, 1, 255, cv2.THRESH_BINARY)\n",
    "    return np.reshape(observation,(84, 84, 1))\n",
    "\n",
    "\n",
    "action0 = 0  # do nothing\n",
    "observation0, reward0, terminal, info = env.step(action0)\n",
    "print(\"Before processing: \" + str(np.array(observation0).shape))\n",
    "plt.imshow(np.array(observation0))\n",
    "plt.show()\n",
    "observation0 = preprocess(observation0)\n",
    "print(\"After processing: \" + str(np.array(observation0).shape))\n",
    "plt.imshow(np.array(np.squeeze(observation0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Double_DQN():\n",
    "    def __init__(self,\n",
    "                 output_size):\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.build_networks()\n",
    "        \n",
    "    def build_networks():\n",
    "        self.primary_network = build_model()\n",
    "        self.target_network = build_model()\n",
    "        \n",
    "    def build_model():\n",
    "        self.input_layer = tf.placeholder([None, 84, 84, 3], dtype = tf.float32)\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=input_layer,\n",
    "            filters=32,\n",
    "            kernel_size=[8, 8],\n",
    "            strides=[4, 4],\n",
    "            padding=\"valid\",\n",
    "            activation=tf.nn.relu)\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=conv1,\n",
    "            filters=64,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"valid\",\n",
    "            activation=tf.nn.relu)\n",
    "        \n",
    "        conv3 = tf.layers.conv2d(\n",
    "            inputs=conv2,\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=tf.nn.relu)\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(\n",
    "            inputs=conv3,\n",
    "            filters=self.output_size,\n",
    "            kernel_size=[7, 7],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=tf.nn.relu)\n",
    "        \n",
    "#         #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "#         streamAC, streamVC = tf.split(conv4, 2, 3)\n",
    "#         self.streamA = slim.flatten(self.streamAC)\n",
    "#         self.streamV = slim.flatten(self.streamVC)\n",
    "#         xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "#         self.AW = tf.Variable(xavier_init([h_size // 2, env.actions]))\n",
    "#         self.VW = tf.Variable(xavier_init([h_size // 2, 1]))\n",
    "#         self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "#         self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "#         #Then combine them together to get our final Q-values.\n",
    "#         self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "#         self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        self.out = conv4\n",
    "        self.predict = tf.argmax(self.out, 1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.out, self.actions_onehot), axis=1)\n",
    "        self.loss = tf.reduce_mean(tf.square(self.targetQ - self.Q))\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "gamma = .99 #Discount factor on the target Q-values\n",
    "\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_episode_len = 50 #The max allowed length of our episode.\n",
    "path = \"../dqn_models\" #The path to save our model to.\n",
    "output_dim = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "start = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    agent = Double_DQN(output_dim)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        for i in range(num_episodes):\n",
    "#             episodeBuffer = experience_buffer()\n",
    "            #Reset environment and get first new observation\n",
    "            s = env.reset()\n",
    "            s = preprocess(s)\n",
    "            d = False\n",
    "            rAll = 0\n",
    "            j = 0\n",
    "            #The Q-Network\n",
    "            while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "                j+=1\n",
    "                #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                    a = np.random.randint(0,4)\n",
    "                else:\n",
    "                    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "                s1,r,d = env.step(a)\n",
    "                s1 = processState(s1)\n",
    "                total_steps += 1\n",
    "                episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "\n",
    "                if total_steps > pre_train_steps:\n",
    "                    if e > endE:\n",
    "                        e -= stepDrop\n",
    "\n",
    "                    if total_steps % (update_freq) == 0:\n",
    "                        trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                        #Below we perform the Double-DQN update to the target Q-values\n",
    "                        Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                        Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                        end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                        doubleQ = Q2[range(batch_size),Q1]\n",
    "                        targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                        #Update the network with our target values.\n",
    "                        _ = sess.run(mainQN.updateModel, \\\n",
    "                            feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "\n",
    "                        updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "                rAll += r\n",
    "                s = s1\n",
    "\n",
    "                if d == True:\n",
    "\n",
    "                    break\n",
    "\n",
    "            myBuffer.add(episodeBuffer.buffer)\n",
    "            jList.append(j)\n",
    "            rList.append(rAll)\n",
    "            #Periodically save the model. \n",
    "            if i % 1000 == 0:\n",
    "                saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "                print(\"Saved Model\")\n",
    "            if len(rList) % 10 == 0:\n",
    "                print(total_steps,np.mean(rList[-10:]), e)\n",
    "        saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "    print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
